---
title: "SparkR"
output: html_document
---

# SparkR

SparkR est une autre package pour utiliser la bibliothèque Apache Spark. Pour lancer le cluster, il
faut utiliser la commande

```{r}
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
``` 

`SPARK_HOME` est une variable d'environement qui désigne le lieu d'installation de la bibliothèque Spark.

## DataFrame SparkR

```{r}
df <- as.DataFrame(faithful)
waiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting))
head(arrange(waiting_counts, desc(waiting_counts$count)))
```

## Exemple de modèle de régression avec MLib

```{r}
data.path <- file.path(Sys.getenv("SPARK_HOME"), "data", "mllib","sample_multiclass_classification_data.txt")

training <- read.df(data.path, source = "libsvm")
# Fit a generalized linear model of family "gaussian" with spark.glm
df_list <- randomSplit(training, c(7, 3), 2)
gaussianDF <- df_list[[1]]
gaussianTestDF <- df_list[[2]]
gaussianGLM <- spark.glm(gaussianDF, label ~ features, family = "gaussian")

# Model summary
summary(gaussianGLM)

# Prediction
gaussianPredictions <- predict(gaussianGLM, gaussianTestDF)
head(gaussianPredictions)
```